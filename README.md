# practical-ml-papers
Unordered list of practical papers I found interesting, I'll keep on adding.

* [A systematic study of the class imbalance problem in convolutional neural networks](https://arxiv.org/abs/1710.05381)
* [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)
* [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/abs/1611.09326)
* [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)
* [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
* [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321)
* [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)
* [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
* [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)
* [FORECASTING ECONOMIC AND FINANCIAL TIME SERIES: ARIMA VS. LSTM](https://arxiv.org/ftp/arxiv/papers/1803/1803.06386.pdf)
 <br><br/>
* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
* [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321)
* [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)
* [PyTorch: An Imperative Style, High-Performance Deep Learning Library](https://arxiv.org/abs/1912.01703?fbclid=IwAR2RnDjn9WBu8SwOgezv6-IgTvI4wINJibWhDTI5wozZjVYxGdF6--iNuHo)
* [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)
* [StyleGAN2 Distillation for Feed-forward Image Manipulation](https://arxiv.org/abs/2003.03581?fbclid=IwAR0VVA6eE8Vzj59_m9sAQLX01d_KV2JAPF97gtxHSkf-DsyHvznMCuwPrIs)
